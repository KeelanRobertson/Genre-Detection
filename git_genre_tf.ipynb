{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3214027-a2c9-49d6-9a6b-e62d4ca7b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import scipy.stats.stats as stats\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14b3709d-cdfa-4e05-8b4b-80c8f7a8faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bba1118f-62d5-4f11-a7c8-9b1546ad1075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "842fcfc5-1657-4d77-8fc7-e5d8ed0fd444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aadf13a6-8471-4e27-8f2a-70df321c4585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ed6e34e5-e10e-49b6-8fdd-a34601351126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.constraints import max_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "dd5436b8-56e0-46b2-b31e-231eed7ac6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all images into memory\n",
    "def load_dataset(path):\n",
    "    photos = list()\n",
    "    targets = list()\n",
    "    genres = \"Folk Rock Instrumental Pop Hip-Hop Electronic International Experimental\"\n",
    "    genres = genres.split()\n",
    "    # enumerate files in the directory\n",
    "    for g in genres:\n",
    "        for filename in os.listdir(path+g):\n",
    "            # load image\n",
    "            photo = image.load_img(path + g + \"/\" + filename, target_size=(72,108,3), color_mode='rgb')\n",
    "            # convert to numpy array\n",
    "            photo = image.img_to_array(photo, dtype='uint8')\n",
    "\n",
    "            # store\n",
    "            photos.append(photo)\n",
    "            targets.append(g)\n",
    "\n",
    "    X = np.asarray(photos, dtype='uint8')\n",
    "    y = np.asarray(targets)\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    return X[indices], y[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "30f4f6d4-e4e9-43ec-b48c-36f0f18557e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_dataset('./content/spectrograms3sec/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "fd6a29be-c6f2-44f4-8f9d-858426bd3458",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = load_dataset('./content/spectrograms3sec/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1b4a79e8-e7cb-4414-a69e-3d46c7e0c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std(x, channels_first=False, verbose=0):\n",
    "    \"\"\"\n",
    "    Calculates channel-wise mean and std\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array\n",
    "        Array representing a collection of images (frames) or\n",
    "        collection of collections of images (frames) - namely video\n",
    "    channels_first : bool, optional\n",
    "        Leave False, by default False\n",
    "    verbose : int, optional\n",
    "        1-prints out details, 0-silent mode, by default 0\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    array of shape [2, num_channels]\n",
    "        Array with per channel mean and std for all the frames\n",
    "    \"\"\"\n",
    "    ndim = x.ndim\n",
    "    assert ndim in [5,4]\n",
    "    assert channels_first == False\n",
    "    all_mean = []\n",
    "    all_std = []    \n",
    "    num_channels = x.shape[-1]\n",
    "    \n",
    "    for c in range(0, num_channels):\n",
    "        if ndim ==5: # videos\n",
    "            mean = x[:,:,:,:,c].mean()\n",
    "            std = x[:,:,:,:,c].std()\n",
    "        elif ndim ==4: # images rgb or grayscale\n",
    "            mean = x[:,:,:,c].mean()\n",
    "            std = x[:,:,:,c].std()\n",
    "        if verbose:\n",
    "            print(\"Channel %s mean before: %s\" % (c, mean))   \n",
    "            print(\"Channel %s std before: %s\" % (c, std))\n",
    "            \n",
    "        all_mean.append(mean)\n",
    "        all_std.append(std)\n",
    "    \n",
    "    return np.stack((all_mean, all_std))\n",
    "\n",
    "\n",
    "def preprocess_input(x, mean_std, divide_std=False, channels_first=False, verbose=0):\n",
    "    \"\"\"\n",
    "    Channel-wise substraction of mean from the input and optional division by std\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array\n",
    "        Input array of images (frames) or videos\n",
    "    mean_std : array\n",
    "        Array of shape [2, num_channels] with per-channel mean and std\n",
    "    divide_std : bool, optional\n",
    "        Add division by std or not, by default False\n",
    "    channels_first : bool, optional\n",
    "        Leave False, otherwise not implemented, by default False\n",
    "    verbose : int, optional\n",
    "        1-prints out details, 0-silent mode, by default 0\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        Returns input array after applying preprocessing steps\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=np.float32)    \n",
    "    ndim = x.ndim\n",
    "    assert ndim in [5,4]\n",
    "    assert channels_first == False\n",
    "    num_channels = x.shape[-1]\n",
    "    \n",
    "    for c in range(0, num_channels):  \n",
    "        if ndim ==5: # videos\n",
    "            x[:,:,:,:,c] -= mean_std[0][c]\n",
    "            if divide_std:\n",
    "                x[:,:,:,:,c] /= mean_std[1][c]\n",
    "            if verbose:\n",
    "                print(\"Channel %s mean after preprocessing: %s\" % (c, x[:,:,:,:,c].mean()))    \n",
    "                print(\"Channel %s std after preprocessing: %s\" % (c, x[:,:,:,:,c].std()))\n",
    "        elif ndim ==4: # images rgb or grayscale\n",
    "            x[:,:,:,c] -= mean_std[0][c]\n",
    "            if divide_std:\n",
    "                x[:,:,:,c] /= mean_std[1][c]   \n",
    "            if verbose:        \n",
    "                print(\"Channel %s mean after preprocessing: %s\" % (c, x[:,:,:,c].mean()))    \n",
    "                print(\"Channel %s std after preprocessing: %s\" % (c, x[:,:,:,c].std()))            \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "761dce49-56cc-453e-b3ae-37eb8b15a1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_std = calculate_mean_std(X_train)\n",
    "X_train = preprocess_input(X_train, mean_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "bc61c6ff-4cf1-4502-a996-8346e2a8644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_std = calculate_mean_std(X_test)\n",
    "X_test = preprocess_input(X_test, mean_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "54d23b7c-8833-4d07-9213-f66abb8b8434",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = LabelEncoder()\n",
    "converter.fit(y_train)\n",
    "y_train = converter.transform(y_train)\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "a8a69ab2-2474-46bf-83da-5e694981b3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = converter.transform(y_test)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77bcf04-38d6-4917-818a-68d3fc3499f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "cd326aa4-ed7d-4cd5-a52d-b8c17b1344f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "2e735e61-3609-44f7-a72e-07a765545d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 70, 106, 32)       896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 68, 104, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 68, 104, 64)       256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 34, 52, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 50, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 50, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 23, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 21, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 12, 21, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 10, 256)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 15360)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               7864832   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 1032      \n",
      "=================================================================\n",
      "Total params: 8,360,968\n",
      "Trainable params: 8,358,920\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "conv_net = Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(72, 108, 3)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer='l2'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    #tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer='l2'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_regularizer='l2'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "    tf.keras.layers.Dense(8, activation='softmax'),\n",
    "])\n",
    "print(conv_net.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "45354327-c387-490e-aba3-c05a7662144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ccf1f1f0-cb67-4b94-9c04-87cbd23c4da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a callback to prevent overfitting\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "ef8b2f31-d572-4938-9d72-290a90a8ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.00005)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "conv_net.compile(optimizer=optimizer,\n",
    "              loss=loss,\n",
    "              metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "b91b4086-04c4-438e-b8ec-7928e45f19c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vijal\\.conda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3350: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "25/25 [==============================] - 4s 177ms/step - loss: 4.3853 - accuracy: 0.2728 - val_loss: 7.4180 - val_accuracy: 0.0926\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 4s 166ms/step - loss: 3.7034 - accuracy: 0.4444 - val_loss: 4.9344 - val_accuracy: 0.0956\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 5s 190ms/step - loss: 3.4078 - accuracy: 0.5422 - val_loss: 4.5543 - val_accuracy: 0.1660\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 4s 173ms/step - loss: 3.1555 - accuracy: 0.6366 - val_loss: 4.2571 - val_accuracy: 0.2213\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 4s 171ms/step - loss: 2.9640 - accuracy: 0.7066 - val_loss: 4.3629 - val_accuracy: 0.1841\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 4s 171ms/step - loss: 2.7770 - accuracy: 0.7834 - val_loss: 4.2472 - val_accuracy: 0.2123\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 4s 169ms/step - loss: 2.6410 - accuracy: 0.8397 - val_loss: 4.0645 - val_accuracy: 0.2736\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 4s 170ms/step - loss: 2.5151 - accuracy: 0.8925 - val_loss: 4.0028 - val_accuracy: 0.3058\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 4s 170ms/step - loss: 2.4158 - accuracy: 0.9281 - val_loss: 3.8843 - val_accuracy: 0.3702\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 4s 170ms/step - loss: 2.3450 - accuracy: 0.9466 - val_loss: 3.8388 - val_accuracy: 0.3793\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 4s 169ms/step - loss: 2.2871 - accuracy: 0.9609 - val_loss: 3.7529 - val_accuracy: 0.3873\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 4s 170ms/step - loss: 2.2254 - accuracy: 0.9772 - val_loss: 3.6584 - val_accuracy: 0.4376\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 4s 169ms/step - loss: 2.1844 - accuracy: 0.9853 - val_loss: 3.5914 - val_accuracy: 0.4507\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 4s 170ms/step - loss: 2.1532 - accuracy: 0.9891 - val_loss: 3.5854 - val_accuracy: 0.4487\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 4s 172ms/step - loss: 2.1268 - accuracy: 0.9941 - val_loss: 3.5105 - val_accuracy: 0.4748\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 4s 178ms/step - loss: 2.0942 - accuracy: 0.9947 - val_loss: 3.3855 - val_accuracy: 0.5080\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 4s 171ms/step - loss: 2.0709 - accuracy: 0.9966 - val_loss: 3.4433 - val_accuracy: 0.4869\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 4s 175ms/step - loss: 2.0485 - accuracy: 0.9972 - val_loss: 3.3755 - val_accuracy: 0.4960\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 4s 174ms/step - loss: 2.0329 - accuracy: 0.9953 - val_loss: 3.3796 - val_accuracy: 0.5241\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 4s 172ms/step - loss: 2.0123 - accuracy: 0.9978 - val_loss: 3.3305 - val_accuracy: 0.5241\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 4s 174ms/step - loss: 1.9950 - accuracy: 0.9966 - val_loss: 3.3590 - val_accuracy: 0.5272\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 4s 174ms/step - loss: 1.9737 - accuracy: 0.9984 - val_loss: 3.3221 - val_accuracy: 0.5322\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 4s 171ms/step - loss: 1.9557 - accuracy: 0.9978 - val_loss: 3.2787 - val_accuracy: 0.5302\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 4s 174ms/step - loss: 1.9364 - accuracy: 0.9997 - val_loss: 3.2943 - val_accuracy: 0.5262\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 4s 174ms/step - loss: 1.9213 - accuracy: 0.9987 - val_loss: 3.2948 - val_accuracy: 0.5362\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 4s 174ms/step - loss: 1.9057 - accuracy: 0.9997 - val_loss: 3.2593 - val_accuracy: 0.5352\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 4s 173ms/step - loss: 1.8866 - accuracy: 0.9994 - val_loss: 3.2613 - val_accuracy: 0.5392\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 4s 172ms/step - loss: 1.8692 - accuracy: 0.9997 - val_loss: 3.2453 - val_accuracy: 0.5463\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 4s 171ms/step - loss: 1.8525 - accuracy: 0.9997 - val_loss: 3.2259 - val_accuracy: 0.5423\n",
      "Epoch 30/50\n",
      "25/25 [==============================] - 4s 170ms/step - loss: 1.8342 - accuracy: 1.0000 - val_loss: 3.2114 - val_accuracy: 0.5533\n",
      "Epoch 31/50\n",
      "25/25 [==============================] - 4s 170ms/step - loss: 1.8195 - accuracy: 0.9994 - val_loss: 3.1951 - val_accuracy: 0.5473\n",
      "Epoch 32/50\n",
      "25/25 [==============================] - 4s 170ms/step - loss: 1.8037 - accuracy: 0.9997 - val_loss: 3.1902 - val_accuracy: 0.5362\n",
      "Epoch 33/50\n",
      "25/25 [==============================] - 4s 170ms/step - loss: 1.7893 - accuracy: 1.0000 - val_loss: 3.1830 - val_accuracy: 0.5412\n",
      "Epoch 34/50\n",
      "25/25 [==============================] - 4s 171ms/step - loss: 1.7723 - accuracy: 0.9994 - val_loss: 3.1804 - val_accuracy: 0.5412\n",
      "Epoch 35/50\n",
      "25/25 [==============================] - 4s 171ms/step - loss: 1.7567 - accuracy: 1.0000 - val_loss: 3.1858 - val_accuracy: 0.5423\n",
      "Epoch 36/50\n",
      "25/25 [==============================] - 4s 170ms/step - loss: 1.7405 - accuracy: 0.9997 - val_loss: 3.1165 - val_accuracy: 0.5392\n",
      "Epoch 37/50\n",
      "25/25 [==============================] - 4s 173ms/step - loss: 1.7239 - accuracy: 0.9997 - val_loss: 3.0999 - val_accuracy: 0.5463\n",
      "Epoch 38/50\n",
      "25/25 [==============================] - 4s 176ms/step - loss: 1.7080 - accuracy: 1.0000 - val_loss: 3.1176 - val_accuracy: 0.5473\n",
      "Epoch 39/50\n",
      "25/25 [==============================] - 4s 173ms/step - loss: 1.6920 - accuracy: 1.0000 - val_loss: 3.1470 - val_accuracy: 0.5453\n",
      "Epoch 40/50\n",
      "25/25 [==============================] - 4s 172ms/step - loss: 1.6739 - accuracy: 1.0000 - val_loss: 3.0718 - val_accuracy: 0.5584\n",
      "Epoch 41/50\n",
      "25/25 [==============================] - 4s 171ms/step - loss: 1.6581 - accuracy: 1.0000 - val_loss: 3.0911 - val_accuracy: 0.5493\n",
      "Epoch 42/50\n",
      "25/25 [==============================] - 4s 171ms/step - loss: 1.6420 - accuracy: 1.0000 - val_loss: 3.0656 - val_accuracy: 0.5443\n",
      "Epoch 43/50\n",
      "25/25 [==============================] - 4s 170ms/step - loss: 1.6258 - accuracy: 1.0000 - val_loss: 3.0200 - val_accuracy: 0.5533\n",
      "Epoch 44/50\n",
      "25/25 [==============================] - 4s 173ms/step - loss: 1.6087 - accuracy: 1.0000 - val_loss: 3.0301 - val_accuracy: 0.5443\n",
      "Epoch 45/50\n",
      "25/25 [==============================] - 4s 170ms/step - loss: 1.5926 - accuracy: 1.0000 - val_loss: 3.0310 - val_accuracy: 0.5473\n",
      "Epoch 46/50\n",
      "25/25 [==============================] - 4s 170ms/step - loss: 1.5768 - accuracy: 1.0000 - val_loss: 2.9908 - val_accuracy: 0.5493\n",
      "Epoch 47/50\n",
      "25/25 [==============================] - 4s 170ms/step - loss: 1.5595 - accuracy: 1.0000 - val_loss: 3.0124 - val_accuracy: 0.5423\n",
      "Epoch 48/50\n",
      "25/25 [==============================] - 4s 170ms/step - loss: 1.5437 - accuracy: 1.0000 - val_loss: 2.9997 - val_accuracy: 0.5412\n",
      "Epoch 49/50\n",
      "25/25 [==============================] - 4s 172ms/step - loss: 1.5273 - accuracy: 1.0000 - val_loss: 2.9352 - val_accuracy: 0.5553\n",
      "Epoch 50/50\n",
      "25/25 [==============================] - 4s 171ms/step - loss: 1.5132 - accuracy: 1.0000 - val_loss: 2.9453 - val_accuracy: 0.5614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ab1ff91c08>"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_net.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=50, batch_size=128)#, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "90b01b1e-588b-43e1-978f-14de015cb4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = conv_net.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "d6afe743-231d-4f7a-9351-fe203cb5ffbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Electronic       0.57      0.59      0.58       128\n",
      " Experimental       0.30      0.59      0.39        44\n",
      "         Folk       0.55      0.60      0.57       140\n",
      "      Hip-Hop       0.58      0.69      0.63       140\n",
      " Instrumental       0.67      0.61      0.64       134\n",
      "International       0.46      0.42      0.44        92\n",
      "          Pop       0.51      0.19      0.28       146\n",
      "         Rock       0.67      0.75      0.71       170\n",
      "\n",
      "     accuracy                           0.56       994\n",
      "    macro avg       0.54      0.56      0.53       994\n",
      " weighted avg       0.57      0.56      0.55       994\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print a calssification report for the test dataset\n",
    "print(classification_report(converter.inverse_transform(np.argmax(y_test, axis=1)), converter.inverse_transform(np.argmax(pred, axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "14892c46-ceaa-4155-8126-26da0004aa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 75   7   7  20   6   4   4   5]\n",
      " [  3  26   3   5   3   1   1   2]\n",
      " [  4  14  84   3  12  13   4   6]\n",
      " [ 12   4   5  97   3   9   6   4]\n",
      " [ 10  14  14   4  82   3   3   4]\n",
      " [  9   8  11  12   3  39   3   7]\n",
      " [ 14   6  21  23   8  11  28  35]\n",
      " [  5   9   9   4   5   5   6 127]]\n"
     ]
    }
   ],
   "source": [
    "# print a confusion matric for the test dataset\n",
    "print(confusion_matrix(converter.inverse_transform(np.argmax(y_test, axis=1)), converter.inverse_transform(np.argmax(pred, axis=1))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-gputest]",
   "language": "python",
   "name": "conda-env-.conda-gputest-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
