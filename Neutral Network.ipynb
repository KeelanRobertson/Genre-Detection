{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline\n",
    "import librosa as lr\n",
    "import librosa.display\n",
    "import audioread\n",
    "import ffmpeg\n",
    "from glob import glob\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting directory for files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = []\n",
    "path = 'fma_small/fma_small'\n",
    "\n",
    "for folder in sorted(os.listdir(path)):\n",
    "    if folder != '.ipynb_checkpoints':\n",
    "        for file in sorted(os.listdir(path+'/'+folder)):\n",
    "    \n",
    "            directory.append(f'{path}/{folder}/{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading in a mp3 file\n",
    "\n",
    "audio, sr = lr.load(directory[0])\n",
    "\n",
    "time = np.arange(0,len(audio)) / sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neutral Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://towardsdatascience.com/music-genre-recognition-using-convolutional-neural-networks-cnn-part-1-212c6b93da76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "from pydub import AudioSegment\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "small_directory = sample(directory, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Breaking audio into 6, 5 second parts\n",
    "\n",
    "for file in small_directory:\n",
    "    for w in range(0,5):\n",
    "        t1 = 6*(w)*1000\n",
    "        t2 = 6*(w+1)*1000\n",
    "        newAudio = AudioSegment.from_mp3(file)\n",
    "        new = newAudio[t1:t2]\n",
    "        new.export(f'CNN-test/{file[24:-4]}'+'-'+f'{str(w)}.mp3', format=\"mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating the spectrograms \n",
    "\n",
    "for clip in os.listdir('CNN-test/'):\n",
    "    \n",
    "    if clip.endswith('.mp3'):\n",
    "        y,sr = librosa.load('CNN-test/'+clip)\n",
    "\n",
    "        mels = librosa.feature.melspectrogram(y=y,sr=sr)\n",
    "        fig = plt.Figure()\n",
    "        canvas = FigureCanvas(fig)\n",
    "        p = plt.imshow(librosa.power_to_db(mels,ref=np.max))\n",
    "        plt.axis('off')\n",
    "        plt.savefig(f'CNN-test/figures/{clip[:-4]}.png')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading in complete table of: track ID, genre, artist and title\n",
    "\n",
    "genre_table = pd.read_csv('final_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hip-Hop': 0,\n",
       " 'Pop': 1,\n",
       " 'Folk': 2,\n",
       " 'Experimental': 3,\n",
       " 'Rock': 4,\n",
       " 'International': 5,\n",
       " 'Electronic': 6,\n",
       " 'Instrumental': 7}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating a dictionary of all different genres to label the images when we generate them\n",
    "\n",
    "genres = list(genre_table['Genre'].unique())\n",
    "\n",
    "genre_map = {genres[i]: i for i in range(len(genres))}\n",
    "\n",
    "genre_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating dictionary of track ids and their genres\n",
    "\n",
    "track_genre = set()\n",
    "\n",
    "for clip in os.listdir('CNN-test/figures/train/'):\n",
    "        track_genre.add(int(clip[:-6]))\n",
    "    \n",
    "\n",
    "track_genre = list(track_genre)\n",
    "track_genre.sort()\n",
    "track_genre = dict.fromkeys(track_genre, 0)\n",
    "\n",
    "## Using genre table to impute out genre for track ids\n",
    "\n",
    "for i in track_genre.keys():\n",
    "    track_genre[i] = genre_table['Genre'][genre_table['Track ID']==i].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Counting of genres in dictionary\n",
    "\n",
    "genre_count_dict = {}\n",
    "\n",
    "for i in genre_table['Genre'].unique():\n",
    "    genre_count_dict[i] = sum(value == i for value in track_genre.values())\n",
    "    \n",
    "genre_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoding of genre\n",
    "\n",
    "def encoding_genre(track_id):\n",
    "    \n",
    "    return genre_map[track_genre[track_id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all images into memory\n",
    "\n",
    "def load_dataset(path):\n",
    "    photos = list()\n",
    "    targets = list()\n",
    "    \n",
    "    # loop audio spectrograms in directory\n",
    "    for filename in os.listdir(path):\n",
    "        # load image\n",
    "        photo = image.load_img(path + filename, target_size=(72,108,3))\n",
    "        # convert to numpy array\n",
    "        photo = image.img_to_array(photo, dtype='uint8')\n",
    "        # one hot encode tags for labels\n",
    "        target = encoding_genre(int(filename[:-6]))\n",
    "        \n",
    "        # store\n",
    "        photos.append(photo)\n",
    "        targets.append(target)\n",
    "    \n",
    "    X = np.asarray(photos, dtype='float')\n",
    "    y = np.asarray(targets)#, dtype='int')\n",
    "\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating X(image array) and y(genre labels)\n",
    "\n",
    "X, y = load_dataset('CNN-test/figures/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One hot encoding\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test train split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaling\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "train_iterator = datagen.flow(X_train, Y_train, batch_size=32)\n",
    "test_iterator = datagen.flow(X_test, Y_test, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNN - wooooooo\n",
    "# define CNN model\n",
    "# https://blog.clairvoyantsoft.com/music-genre-classification-using-cnn-ef9461553726\n",
    "# https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-satellite-photos-of-the-amazon-rainforest/\n",
    "# https://towardsdatascience.com/music-genre-recognition-using-convolutional-neural-networks-cnn-part-1-212c6b93da76\n",
    "# https://brilliant.org/wiki/convolutional-neural-network/\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\n",
    "# https://www.kaggle.com/andradaolteanu/work-w-audio-data-visualise-classify-recommend\n",
    "\n",
    "\n",
    "def define_model():\n",
    "    \n",
    "    # build model\n",
    "    model = Sequential([\n",
    "    \n",
    "    tf.keras.layers.Conv2D(16, (5, 5), activation='relu', input_shape=(72, 108, 3)),\n",
    "    tf.keras.layers.Conv2D(16, (5, 5), activation='relu', kernel_regularizer='l2'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(32, (5, 5), activation='relu'),\n",
    "    tf.keras.layers.Conv2D(32, (5, 5), activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (5, 5), activation='relu'),\n",
    "    tf.keras.layers.Conv2D(64, (5, 5), activation='relu', kernel_regularizer='l2'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "        \n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "    tf.keras.layers.Dense(8, activation='softmax')])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_84 (Conv2D)           (None, 68, 104, 16)       1216      \n",
      "_________________________________________________________________\n",
      "conv2d_85 (Conv2D)           (None, 64, 100, 16)       6416      \n",
      "_________________________________________________________________\n",
      "batch_normalization_84 (Batc (None, 64, 100, 16)       64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_51 (MaxPooling (None, 32, 50, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 32, 50, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_86 (Conv2D)           (None, 28, 46, 32)        12832     \n",
      "_________________________________________________________________\n",
      "conv2d_87 (Conv2D)           (None, 24, 42, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_85 (Batc (None, 24, 42, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_52 (MaxPooling (None, 12, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 12, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_88 (Conv2D)           (None, 8, 17, 64)         51264     \n",
      "_________________________________________________________________\n",
      "conv2d_89 (Conv2D)           (None, 4, 13, 64)         102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_86 (Batc (None, 4, 13, 64)         256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_53 (MaxPooling (None, 2, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 2, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 64)                49216     \n",
      "_________________________________________________________________\n",
      "batch_normalization_87 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_88 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 8)                 264       \n",
      "=================================================================\n",
      "Total params: 252,216\n",
      "Trainable params: 251,800\n",
      "Non-trainable params: 416\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "92/92 [==============================] - 78s 828ms/step - loss: 3.1745 - accuracy: 0.1802 - val_loss: 2.8467 - val_accuracy: 0.1319\n",
      "Epoch 2/50\n",
      "92/92 [==============================] - 75s 819ms/step - loss: 2.8735 - accuracy: 0.2191 - val_loss: 3.0875 - val_accuracy: 0.1279\n",
      "Epoch 3/50\n",
      "92/92 [==============================] - 75s 814ms/step - loss: 2.6888 - accuracy: 0.2606 - val_loss: 2.7759 - val_accuracy: 0.1279\n",
      "Epoch 4/50\n",
      "92/92 [==============================] - 76s 829ms/step - loss: 2.5726 - accuracy: 0.2814 - val_loss: 2.6471 - val_accuracy: 0.1430\n",
      "Epoch 5/50\n",
      "92/92 [==============================] - 75s 813ms/step - loss: 2.4269 - accuracy: 0.3216 - val_loss: 2.6188 - val_accuracy: 0.1493\n",
      "Epoch 6/50\n",
      "92/92 [==============================] - 75s 813ms/step - loss: 2.3233 - accuracy: 0.3499 - val_loss: 3.1434 - val_accuracy: 0.1644\n",
      "Epoch 7/50\n",
      "92/92 [==============================] - 75s 811ms/step - loss: 2.2481 - accuracy: 0.3598 - val_loss: 2.6280 - val_accuracy: 0.2327\n",
      "Epoch 8/50\n",
      "92/92 [==============================] - 76s 828ms/step - loss: 2.1535 - accuracy: 0.3799 - val_loss: 2.2731 - val_accuracy: 0.2875\n",
      "Epoch 9/50\n",
      "92/92 [==============================] - 75s 817ms/step - loss: 2.0638 - accuracy: 0.3997 - val_loss: 2.3109 - val_accuracy: 0.2153\n",
      "Epoch 10/50\n",
      "92/92 [==============================] - 75s 814ms/step - loss: 2.0067 - accuracy: 0.4089 - val_loss: 2.0810 - val_accuracy: 0.3535\n",
      "Epoch 11/50\n",
      "92/92 [==============================] - 75s 814ms/step - loss: 1.9334 - accuracy: 0.4266 - val_loss: 3.0938 - val_accuracy: 0.2581\n",
      "Epoch 12/50\n",
      "92/92 [==============================] - 78s 843ms/step - loss: 1.9047 - accuracy: 0.4211 - val_loss: 2.1280 - val_accuracy: 0.3169\n",
      "Epoch 13/50\n",
      "92/92 [==============================] - 68s 741ms/step - loss: 1.8411 - accuracy: 0.4392 - val_loss: 2.3474 - val_accuracy: 0.2478\n",
      "Epoch 14/50\n",
      "92/92 [==============================] - 67s 730ms/step - loss: 1.7774 - accuracy: 0.4583 - val_loss: 1.7712 - val_accuracy: 0.4281\n",
      "Epoch 15/50\n",
      "92/92 [==============================] - 68s 735ms/step - loss: 1.7204 - accuracy: 0.4634 - val_loss: 2.6039 - val_accuracy: 0.2566\n",
      "Epoch 16/50\n",
      "92/92 [==============================] - 67s 727ms/step - loss: 1.7103 - accuracy: 0.4828 - val_loss: 2.2875 - val_accuracy: 0.3185\n",
      "Epoch 17/50\n",
      "92/92 [==============================] - 67s 726ms/step - loss: 1.6738 - accuracy: 0.4947 - val_loss: 1.8046 - val_accuracy: 0.4138\n",
      "Epoch 18/50\n",
      "92/92 [==============================] - 66s 722ms/step - loss: 1.5789 - accuracy: 0.5193 - val_loss: 2.0228 - val_accuracy: 0.3670\n",
      "Epoch 19/50\n",
      "92/92 [==============================] - 66s 722ms/step - loss: 1.5481 - accuracy: 0.5370 - val_loss: 2.2842 - val_accuracy: 0.2010\n",
      "Epoch 20/50\n",
      "92/92 [==============================] - 66s 720ms/step - loss: 1.5142 - accuracy: 0.5397 - val_loss: 2.0301 - val_accuracy: 0.3487\n",
      "Epoch 21/50\n",
      "92/92 [==============================] - 67s 728ms/step - loss: 1.4857 - accuracy: 0.5407 - val_loss: 2.0536 - val_accuracy: 0.3900\n",
      "Epoch 22/50\n",
      "92/92 [==============================] - 4057s 45s/step - loss: 1.4595 - accuracy: 0.5516 - val_loss: 1.6829 - val_accuracy: 0.4376\n",
      "Epoch 23/50\n",
      "92/92 [==============================] - 77s 839ms/step - loss: 1.3659 - accuracy: 0.5945 - val_loss: 1.7789 - val_accuracy: 0.4575\n",
      "Epoch 24/50\n",
      "92/92 [==============================] - 75s 816ms/step - loss: 1.3054 - accuracy: 0.6211 - val_loss: 1.9871 - val_accuracy: 0.3630\n",
      "Epoch 25/50\n",
      "92/92 [==============================] - 76s 822ms/step - loss: 1.2873 - accuracy: 0.6208 - val_loss: 2.1219 - val_accuracy: 0.3828\n",
      "Epoch 26/50\n",
      "92/92 [==============================] - 76s 829ms/step - loss: 1.2401 - accuracy: 0.6378 - val_loss: 1.9161 - val_accuracy: 0.3717\n",
      "Epoch 27/50\n",
      "92/92 [==============================] - 75s 818ms/step - loss: 1.2023 - accuracy: 0.6552 - val_loss: 2.2467 - val_accuracy: 0.3876\n",
      "Epoch 28/50\n",
      "92/92 [==============================] - 75s 814ms/step - loss: 1.1613 - accuracy: 0.6627 - val_loss: 1.7049 - val_accuracy: 0.4813\n",
      "Epoch 29/50\n",
      "92/92 [==============================] - 76s 822ms/step - loss: 1.1351 - accuracy: 0.6879 - val_loss: 2.2791 - val_accuracy: 0.3948\n",
      "Epoch 30/50\n",
      "92/92 [==============================] - 75s 814ms/step - loss: 1.0652 - accuracy: 0.7155 - val_loss: 1.5699 - val_accuracy: 0.5552\n",
      "Epoch 31/50\n",
      "92/92 [==============================] - 75s 816ms/step - loss: 1.0064 - accuracy: 0.7312 - val_loss: 1.7190 - val_accuracy: 0.5155\n",
      "Epoch 32/50\n",
      "92/92 [==============================] - 75s 815ms/step - loss: 0.9840 - accuracy: 0.7441 - val_loss: 2.0182 - val_accuracy: 0.4551\n",
      "Epoch 33/50\n",
      "92/92 [==============================] - 77s 840ms/step - loss: 0.9501 - accuracy: 0.7509 - val_loss: 2.5929 - val_accuracy: 0.4091\n",
      "Epoch 34/50\n",
      "92/92 [==============================] - 90s 977ms/step - loss: 0.9293 - accuracy: 0.7598 - val_loss: 2.0332 - val_accuracy: 0.4543\n",
      "Epoch 35/50\n",
      "92/92 [==============================] - 87s 947ms/step - loss: 0.8874 - accuracy: 0.7758 - val_loss: 2.2457 - val_accuracy: 0.3836\n",
      "Epoch 36/50\n",
      "92/92 [==============================] - 87s 945ms/step - loss: 0.8715 - accuracy: 0.7922 - val_loss: 2.1368 - val_accuracy: 0.4527\n",
      "Epoch 37/50\n",
      "92/92 [==============================] - 88s 959ms/step - loss: 0.8480 - accuracy: 0.8020 - val_loss: 1.6793 - val_accuracy: 0.5520\n",
      "Epoch 38/50\n",
      "92/92 [==============================] - 88s 963ms/step - loss: 0.7696 - accuracy: 0.8310 - val_loss: 1.9463 - val_accuracy: 0.4893\n",
      "Epoch 39/50\n",
      "92/92 [==============================] - 87s 942ms/step - loss: 0.7320 - accuracy: 0.8422 - val_loss: 2.0656 - val_accuracy: 0.4543\n",
      "Epoch 40/50\n",
      "92/92 [==============================] - 87s 946ms/step - loss: 0.7729 - accuracy: 0.8266 - val_loss: 1.8315 - val_accuracy: 0.5393\n",
      "Epoch 41/50\n",
      "92/92 [==============================] - 88s 959ms/step - loss: 0.7214 - accuracy: 0.8491 - val_loss: 2.0122 - val_accuracy: 0.5091\n",
      "Epoch 42/50\n",
      "92/92 [==============================] - 85s 926ms/step - loss: 0.7018 - accuracy: 0.8532 - val_loss: 1.8902 - val_accuracy: 0.5218\n",
      "Epoch 43/50\n",
      "92/92 [==============================] - 85s 928ms/step - loss: 0.6468 - accuracy: 0.8698 - val_loss: 2.2292 - val_accuracy: 0.4408\n",
      "Epoch 44/50\n",
      "92/92 [==============================] - 86s 933ms/step - loss: 0.6488 - accuracy: 0.8705 - val_loss: 1.6934 - val_accuracy: 0.5862\n",
      "Epoch 45/50\n",
      "92/92 [==============================] - 413s 5s/step - loss: 0.6444 - accuracy: 0.8746 - val_loss: 1.6604 - val_accuracy: 0.5774\n",
      "Epoch 46/50\n",
      "92/92 [==============================] - 94s 1s/step - loss: 0.5991 - accuracy: 0.8893 - val_loss: 1.8176 - val_accuracy: 0.5902\n",
      "Epoch 47/50\n",
      "92/92 [==============================] - 98s 1s/step - loss: 0.5869 - accuracy: 0.8995 - val_loss: 2.7074 - val_accuracy: 0.4631\n",
      "Epoch 48/50\n",
      "92/92 [==============================] - 97s 1s/step - loss: 0.6010 - accuracy: 0.8845 - val_loss: 1.9838 - val_accuracy: 0.5052\n",
      "Epoch 49/50\n",
      "92/92 [==============================] - 99s 1s/step - loss: 0.5869 - accuracy: 0.8988 - val_loss: 1.6910 - val_accuracy: 0.5870\n",
      "Epoch 50/50\n",
      "92/92 [==============================] - 96s 1s/step - loss: 0.5274 - accuracy: 0.9121 - val_loss: 2.5232 - val_accuracy: 0.4893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x12e11830b80>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_iterator, validation_data = test_iterator, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Hip-Hop': 0,\n",
    "# 'Pop': 1, \n",
    "# 'Folk': 2,\n",
    "# 'Experimental': 3,\n",
    "# 'Rock': 4,\n",
    "# 'International': 5,\n",
    "# 'Electronic': 6,\n",
    "# 'Instrumental': 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking class balance in Y_test\n",
    "\n",
    "genre_count_dict = {}\n",
    "\n",
    "for i in range(0,8):\n",
    "    genre_count_dict[i] = sum(value == i for value in Y_train)\n",
    "    \n",
    "genre_count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "genre_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLearn packages - neutral net works better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Forrest\n",
    "## Logistic Regression\n",
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmeta = pd.read_csv('MetaDataBetter.csv', header=[0,1,2], index_col=0)\n",
    "\n",
    "dfmeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding genre and label encoding\n",
    "\n",
    "genre = pd.read_csv('FinalGenre.csv',index_col=0)  \n",
    "\n",
    "\n",
    "dfmeta = dfmeta.join(genre, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmeta.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XRF = dfmeta.iloc[:,:-8]\n",
    "YRF = dfmeta.loc[:,'Genre']\n",
    "\n",
    "X_train_RF, X_test_RF, Y_train_RF, Y_test_RF = train_test_split(XRF, YRF, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scaler(x):\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = scaler.fit_transform(x)\n",
    "    \n",
    "    \n",
    "    return scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_RF = scaler(X_train_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = {\n",
    "    'n_estimators': [100, 150],\n",
    "    'max_depth': [15]\n",
    "                }\n",
    "\n",
    "\n",
    "RF = GridSearchCV(RandomForestClassifier(),param_grid = tuned_parameters, cv = 5, verbose=3, scoring='accuracy')\n",
    "\n",
    "RF.fit(X_train_RF,Y_train_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = RF.predict(X_test_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(Y_test_RF, pred))\n",
    "print(confusion_matrix(Y_test_RF, pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
